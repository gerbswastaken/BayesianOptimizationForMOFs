{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "center-mathematics",
   "metadata": {},
   "source": [
    "# BO runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97594ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting botorch\n",
      "  Downloading botorch-0.8.2-py3-none-any.whl (520 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.0/521.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.12 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from botorch) (1.13.1)\n",
      "Collecting pyro-ppl>=1.8.4\n",
      "  Using cached pyro_ppl-1.8.4-py3-none-any.whl (730 kB)\n",
      "Collecting multipledispatch\n",
      "  Using cached multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting gpytorch==1.9.1\n",
      "  Using cached gpytorch-1.9.1-py3-none-any.whl (250 kB)\n",
      "Requirement already satisfied: scipy in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from botorch) (1.7.1)\n",
      "Collecting linear-operator==0.3.0\n",
      "  Using cached linear_operator-0.3.0-py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from gpytorch==1.9.1->botorch) (0.24.2)\n",
      "Collecting pyro-api>=0.1.1\n",
      "  Using cached pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from pyro-ppl>=1.8.4->botorch) (1.22.4)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tqdm>=4.36\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from torch>=1.12->botorch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from torch>=1.12->botorch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from torch>=1.12->botorch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from torch>=1.12->botorch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from torch>=1.12->botorch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12->botorch) (58.2.0)\n",
      "Requirement already satisfied: wheel in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.12->botorch) (0.37.0)\n",
      "Requirement already satisfied: six in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from multipledispatch->botorch) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from scikit-learn->gpytorch==1.9.1->botorch) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages (from scikit-learn->gpytorch==1.9.1->botorch) (1.2.0)\n",
      "Installing collected packages: pyro-api, tqdm, opt-einsum, multipledispatch, pyro-ppl, linear-operator, gpytorch, botorch\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-nightly 2.12.0.dev20221110 requires astunparse>=1.6.0, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires gast<=0.4.0,>=0.2.1, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires google-pasta>=0.1.1, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires grpcio<2.0,>=1.24.3, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires h5py>=2.9.0, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires termcolor>=1.1.0, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires wrapt>=1.11.0, which is not installed.\n",
      "tf-nightly 2.12.0.dev20221110 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botorch-0.8.2 gpytorch-1.9.1 linear-operator-0.3.0 multipledispatch-0.6.0 opt-einsum-3.3.0 pyro-api-0.1.2 pyro-ppl-1.8.4 tqdm-4.65.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ambient-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.models import FixedNoiseGP, SingleTaskGP\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "from scipy.stats import norm\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-flour",
   "metadata": {},
   "source": [
    "load data from `prepare_Xy.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comic-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (98694, 7)\n",
      "shape of y: (98694, 1)\n",
      "# COFs: 98694\n",
      "# iterations: 100\n",
      "# runs: 50\n"
     ]
    }
   ],
   "source": [
    "X = pickle.load(open('hydrogen_input_output.pkl', 'rb'))['x']\n",
    "print(\"shape of X:\", np.shape(X))\n",
    "\n",
    "y = pickle.load(open('hydrogen_input_output.pkl', 'rb'))['y']\n",
    "y = np.reshape(y, (np.size(y), 1)) # for the GP\n",
    "print(\"shape of y:\", np.shape(y))\n",
    "\n",
    "nb_COFs = pickle.load(open('hydrogen_input_output.pkl', 'rb'))['nb_COFs']\n",
    "print(\"# COFs:\", nb_COFs)\n",
    "\n",
    "nb_iterations = pickle.load(open('hydrogen_input_output.pkl', 'rb'))['nb_iterations']\n",
    "print(\"# iterations:\", nb_iterations)\n",
    "\n",
    "nb_runs = pickle.load(open('hydrogen_input_output.pkl', 'rb'))['nb_runs']\n",
    "print(\"# runs:\", nb_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-walter",
   "metadata": {},
   "source": [
    "convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "seventh-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subtle-holmes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98694, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limiting-collar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98694, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resistant-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unsqueezed = X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "earned-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BO run.\n",
    "\n",
    "* nb_iterations: # of COFs to acquire = # of iterations in BO\n",
    "* nb_COFs_initialization: # of COFs to acquire at random to initiate the GP and BO.\n",
    "* which_acquisition: the acquisition function to use.\n",
    "* store_explore_exploit_terms: True if you want to store (exploration contribution, exploitation contribution) to EI\n",
    "\"\"\"\n",
    "def bo_run(nb_iterations, nb_COFs_initialization, which_acquisition, verbose=False, store_explore_exploit_terms=False):\n",
    "    assert nb_iterations > nb_COFs_initialization\n",
    "    assert which_acquisition in ['max y_hat', 'EI', 'max sigma']\n",
    "    \n",
    "    # select initial COFs for training data randomly.\n",
    "    # idea is to keep populating this ids_acquired and return it for analysis.\n",
    "    ids_acquired = np.random.choice(np.arange((nb_COFs)), size=nb_COFs_initialization, replace=False)\n",
    "    \n",
    "    # keep track of exploration vs. exploitation terms ONLY for when using EI  \n",
    "    if which_acquisition == \"EI\" and store_explore_exploit_terms:\n",
    "        explore_exploit_balance = np.array([(np.NaN, np.NaN) for i in range(nb_iterations)])\n",
    "    else:\n",
    "        explore_exploit_balance = [] # don't bother\n",
    "\n",
    "    # initialize acquired y, since it requires normalization\n",
    "    y_acquired = y[ids_acquired]\n",
    "    # standardize outputs using *only currently acquired data*\n",
    "    y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "    \n",
    "    for i in range(nb_COFs_initialization, nb_iterations):\n",
    "        print(\"iteration:\", i, end=\"\\r\")\n",
    "        # construct and fit GP model\n",
    "        model = SingleTaskGP(X[ids_acquired, :], y_acquired)\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "        # set up acquisition function\n",
    "        if which_acquisition == \"EI\":\n",
    "            acquisition_function = ExpectedImprovement(model, best_f=y_acquired.max().item())\n",
    "            \n",
    "            # if having memory problems, compute EI this way, in batches :)\n",
    "#             batch_size = 35000 # need to do in batches to avoid mem issues\n",
    "#             acquisition_values = torch.zeros((nb_COFs))\n",
    "#             acquisition_values[:] = np.NaN # for safety\n",
    "#             nb_batches = nb_COFs // batch_size\n",
    "#             for ba in range(nb_batches+1):\n",
    "#                 id_start = ba * batch_size\n",
    "#                 id_end   = id_start + batch_size\n",
    "#                 if id_end > nb_COFs:\n",
    "#                     id_end = nb_COFs\n",
    "#                 with torch.no_grad():\n",
    "#                     acquisition_values[id_start:id_end] = acquisition_function.forward(X_unsqueezed[id_start:id_end])\n",
    "#             assert acquisition_values.isnan().sum().item() == 0 # so that all are filled properly.\n",
    "            with torch.no_grad(): # to avoid memory issues; we arent using the gradient...\n",
    "                acquisition_values = acquisition_function.forward(X_unsqueezed) # runs out of memory\n",
    "        elif which_acquisition == \"max y_hat\":\n",
    "            with torch.no_grad():\n",
    "                acquisition_values = model.posterior(X_unsqueezed).mean.squeeze()\n",
    "        elif which_acquisition == \"max sigma\":\n",
    "            with torch.no_grad():\n",
    "                acquisition_values = model.posterior(X_unsqueezed).variance.squeeze()\n",
    "        else:\n",
    "            raise Exception(\"not a valid acquisition function\")\n",
    "\n",
    "        # select COF to acquire with maximal aquisition value, which is not in the acquired set already\n",
    "        ids_sorted_by_aquisition = acquisition_values.argsort(descending=True)\n",
    "        for id_max_aquisition_all in ids_sorted_by_aquisition:\n",
    "            if not id_max_aquisition_all.item() in ids_acquired:\n",
    "                id_max_aquisition = id_max_aquisition_all.item()\n",
    "                break\n",
    "\n",
    "        # acquire this COF\n",
    "        ids_acquired = np.concatenate((ids_acquired, [id_max_aquisition]))\n",
    "        assert np.size(ids_acquired) == i + 1\n",
    "        \n",
    "        # if EI, compute and store explore-exploit terms that contribute to EI separately.\n",
    "        if which_acquisition == \"EI\" and store_explore_exploit_terms:\n",
    "            # explore, exploit terms of EI. requires computing EI manually, essentially. \n",
    "            y_pred = model.posterior(X_unsqueezed[id_max_aquisition]).mean.squeeze().detach().numpy()\n",
    "            sigma_pred = np.sqrt(model.posterior(X_unsqueezed[id_max_aquisition]).variance.squeeze().detach().numpy())\n",
    "            \n",
    "            y_max = y_acquired.max().item()\n",
    "            \n",
    "            z = (y_pred - y_max) / sigma_pred\n",
    "            explore_term = sigma_pred * norm.pdf(z)\n",
    "            exploit_term = (y_pred - y_max) * norm.cdf(z)\n",
    "            \n",
    "            # check we computed it right... i.e. that it agrees with BO torch's EI.\n",
    "            assert np.isclose(explore_term + exploit_term, acquisition_values[id_max_aquisition].item())\n",
    "\n",
    "            explore_exploit_balance[i] = (explore_term, exploit_term)\n",
    "\n",
    "        # update y aquired; start over to normalize properly\n",
    "        y_acquired = y[ids_acquired] # start over to normalize y properly\n",
    "        y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\tacquired COF\", id_max_aquisition, \"with y = \", y[id_max_aquisition].item())\n",
    "            print(\"\\tbest y acquired:\", y[ids_acquired].max().item())\n",
    "        \n",
    "    assert np.size(ids_acquired) == nb_iterations\n",
    "    return ids_acquired, explore_exploit_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-assumption",
   "metadata": {},
   "source": [
    "must run with `which_acquisition` equal to all three below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adapted-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# COFs in initialization: 10\n",
      "\n",
      "RUN 0\n",
      "took time t =  2.491740079720815 min\n",
      "\n",
      "\n",
      "RUN 1\n",
      "took time t =  2.3598769982655843 min\n",
      "\n",
      "\n",
      "RUN 2\n",
      "took time t =  2.2923808137575787 min\n",
      "\n",
      "\n",
      "RUN 3\n",
      "took time t =  2.4538424968719483 min\n",
      "\n",
      "\n",
      "RUN 4\n",
      "took time t =  2.2930290619532268 min\n",
      "\n",
      "\n",
      "RUN 5\n",
      "took time t =  2.452386287848155 min\n",
      "\n",
      "\n",
      "RUN 6\n",
      "took time t =  2.5364050308863324 min\n",
      "\n",
      "\n",
      "RUN 7\n",
      "took time t =  2.28095672527949 min\n",
      "\n",
      "\n",
      "RUN 8\n",
      "took time t =  2.3892273267110187 min\n",
      "\n",
      "\n",
      "RUN 9\n",
      "took time t =  2.3227869510650634 min\n",
      "\n",
      "\n",
      "RUN 10\n",
      "took time t =  2.3925676941871643 min\n",
      "\n",
      "\n",
      "RUN 11\n",
      "took time t =  2.3349881052970884 min\n",
      "\n",
      "\n",
      "RUN 12\n",
      "took time t =  2.339087728659312 min\n",
      "\n",
      "\n",
      "RUN 13\n",
      "took time t =  2.480728538831075 min\n",
      "\n",
      "\n",
      "RUN 14\n",
      "took time t =  2.342625153064728 min\n",
      "\n",
      "\n",
      "RUN 15\n",
      "took time t =  2.28629891872406 min\n",
      "\n",
      "\n",
      "RUN 16\n",
      "took time t =  2.565360959370931 min\n",
      "\n",
      "\n",
      "RUN 17\n",
      "iteration: 84\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-07 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took time t =  2.4627824306488035 min\n",
      "\n",
      "\n",
      "RUN 18\n",
      "took time t =  2.409364386399587 min\n",
      "\n",
      "\n",
      "RUN 19\n",
      "iteration: 99\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-07 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/santanu/anaconda3/envs/sage/lib/python3.8/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took time t =  2.6117578665415446 min\n",
      "\n",
      "\n",
      "RUN 20\n",
      "took time t =  2.315704945723216 min\n",
      "\n",
      "\n",
      "RUN 21\n",
      "took time t =  2.382988409201304 min\n",
      "\n",
      "\n",
      "RUN 22\n",
      "took time t =  2.2967713793118794 min\n",
      "\n",
      "\n",
      "RUN 23\n",
      "took time t =  2.311958742141724 min\n",
      "\n",
      "\n",
      "RUN 24\n",
      "took time t =  2.488028864065806 min\n",
      "\n",
      "\n",
      "RUN 25\n",
      "took time t =  2.3035463213920595 min\n",
      "\n",
      "\n",
      "RUN 26\n",
      "took time t =  2.3117849826812744 min\n",
      "\n",
      "\n",
      "RUN 27\n",
      "took time t =  2.366830825805664 min\n",
      "\n",
      "\n",
      "RUN 28\n",
      "took time t =  2.278993240992228 min\n",
      "\n",
      "\n",
      "RUN 29\n",
      "took time t =  2.1843421936035154 min\n",
      "\n",
      "\n",
      "RUN 30\n",
      "took time t =  2.409311560789744 min\n",
      "\n",
      "\n",
      "RUN 31\n",
      "took time t =  2.3542076071103413 min\n",
      "\n",
      "\n",
      "RUN 32\n",
      "took time t =  2.131477439403534 min\n",
      "\n",
      "\n",
      "RUN 33\n",
      "took time t =  2.244883648554484 min\n",
      "\n",
      "\n",
      "RUN 34\n",
      "took time t =  2.3656041304270428 min\n",
      "\n",
      "\n",
      "RUN 35\n",
      "took time t =  2.3882257024447124 min\n",
      "\n",
      "\n",
      "RUN 36\n",
      "took time t =  2.559925897916158 min\n",
      "\n",
      "\n",
      "RUN 37\n",
      "took time t =  2.4093440731366473 min\n",
      "\n",
      "\n",
      "RUN 38\n",
      "took time t =  2.414399512608846 min\n",
      "\n",
      "\n",
      "RUN 39\n",
      "took time t =  2.225052313009898 min\n",
      "\n",
      "\n",
      "RUN 40\n",
      "took time t =  2.2010485410690306 min\n",
      "\n",
      "\n",
      "RUN 41\n",
      "took time t =  2.5044166326522825 min\n",
      "\n",
      "\n",
      "RUN 42\n",
      "took time t =  2.4738738814989727 min\n",
      "\n",
      "\n",
      "RUN 43\n",
      "took time t =  2.1931128184000652 min\n",
      "\n",
      "\n",
      "RUN 44\n",
      "took time t =  2.3426164507865908 min\n",
      "\n",
      "\n",
      "RUN 45\n",
      "took time t =  2.359823993841807 min\n",
      "\n",
      "\n",
      "RUN 46\n",
      "took time t =  2.4314924756685894 min\n",
      "\n",
      "\n",
      "RUN 47\n",
      "took time t =  2.4673810958862306 min\n",
      "\n",
      "\n",
      "RUN 48\n",
      "took time t =  2.2970547795295717 min\n",
      "\n",
      "\n",
      "RUN 49\n",
      "took time t =  2.2829209009806317 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# which_acquisition = \"EI\"\n",
    "# which_acquisition = \"max y_hat\"\n",
    "which_acquisition = \"max sigma\"\n",
    "nb_COFs_initializations = {\"EI\": [5, 10, 15, 20, 25], \n",
    "                           \"max y_hat\": [10], \n",
    "                           \"max sigma\": [10]}\n",
    "\n",
    "for nb_COFs_initialization in nb_COFs_initializations[which_acquisition]:\n",
    "    print(\"# COFs in initialization:\", nb_COFs_initialization)\n",
    "    # store results here.\n",
    "    bo_res = dict() \n",
    "    bo_res['ids_acquired']            = []\n",
    "    bo_res['explore_exploit_balance'] = []\n",
    "    \n",
    "    if nb_COFs_initialization == 10 and which_acquisition == 'EI':\n",
    "        store_explore_exploit_terms = True\n",
    "    else:\n",
    "        store_explore_exploit_terms = False\n",
    "    \n",
    "    for r in range(nb_runs):\n",
    "        print(\"\\nRUN\", r)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        ids_acquired, explore_exploit_balance = bo_run(nb_iterations, nb_COFs_initialization, which_acquisition, store_explore_exploit_terms=store_explore_exploit_terms)\n",
    "        \n",
    "        # store results from this run.\n",
    "        bo_res['ids_acquired'].append(ids_acquired)\n",
    "        bo_res['explore_exploit_balance'].append(explore_exploit_balance)\n",
    "        \n",
    "        print(\"took time t = \", (time.time() - t0) / 60, \"min\\n\")\n",
    "    \n",
    "    # save results from all runs\n",
    "    with open('bo_results_' + which_acquisition + \"_initiate_with_{0}\".format(nb_COFs_initialization) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(bo_res, file)\n",
    "        \n",
    "with open('bo_results_nb_COF_initializations.pkl', 'wb') as file:\n",
    "    pickle.dump(nb_COFs_initializations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4214a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
